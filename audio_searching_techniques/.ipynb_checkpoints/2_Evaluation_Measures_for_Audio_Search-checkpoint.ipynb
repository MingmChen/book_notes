{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation Measures for Audio Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of **various metrics** widely used for **evaluationg the performance of audio search system** are introduced in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Measures\n",
    "\n",
    "#### Background\n",
    "![](images/2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "#### 1. Accuracy and Error Rate\n",
    "$$\n",
    "    \\mathbf{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Error Rate = 1 - Accuracy = \\frac{FN + FP}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sensitivity and Specificity\n",
    "$$\n",
    "    \\mathbf{Sensitivity} = \\frac{TP}{FN + TP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Specificity = \\frac{TN}{TN + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Precision and Recall\n",
    "$$\n",
    "\\mathbf{Precision} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{recall} = \\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Mean Average Precision(MAP)\n",
    "**MAP**: The mean of the precision scores after each query hit is retrived. **The MAP is roughly the average area under the precision-recall curve for a set queries.**\n",
    "\n",
    "If the set of relevant documents for a query \\\\(q_j \\in Q\\\\) is \\\\(d_1, d_2, ..., d_{m_j}\\\\) and \\\\(R_{jk}\\\\) is the set of ranked retrival results from the top result untill the document \\\\(d_k\\\\) is obtained, then\n",
    "$$\n",
    "    MAP(Q) = \\frac{1}{|Q|}\\sum_{j=1}^{|Q|}\\frac{1}{m_j}\\sum_{k=1}^{m_j} Precision(R_{jk})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Miss Rate and False Alarm Rate\n",
    "$$\n",
    "    MissRate = \\frac{FN}{FN+TP}=1-TruePositiveRate\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mathbf{FalseAlarmRate} = 1 - specificity = \\frac{FP}{FP+TN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. F-Measure\n",
    "F-Measure combines the value of precision and recall to a single number using harmonic mean.\n",
    "\n",
    "$$\n",
    "    \\mathbf{F-Measure} = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}=\\frac{2\\times TP}{2TP + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Reciever Operating Characteristics(ROC)\n",
    "\n",
    "ROC curve is commonly used to examine the **trade-off between the detection of true positives, while avoiding the false postives**.\n",
    "\n",
    "In ROC plot, **the closer the curve to the perfect classifier, the better is the system in identifying true-positive value**. This can be measured using another metrics named **\"Area under ROC\"**.\n",
    "\n",
    "![](images/2_2.png)\n",
    "![](images/2_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Figure of Merit(FOM)\n",
    "\n",
    "A well-establish evaluation metrics used for the KWS task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Detection Error Trade-off(DET)\n",
    "\n",
    "Like ROC, DET ia another visual measure of evaluating the performance of a system.\n",
    "![](images/2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Term Weighted Value(TWV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Word Error Rate(WER)\n",
    "\n",
    "A common metrics for analyzing the performance of a speech recognition system.\n",
    "\n",
    "$$\n",
    "    WER = \\frac{S+D+I}{N}\\times 100\n",
    "$$\n",
    "where:    \n",
    "* \\\\(S\\\\): substitution\n",
    "* \\\\(I\\\\): Insertion\n",
    "* \\\\(D\\\\): Deletion\n",
    "\n",
    "**Substitution Error Rate(SBUR)**\n",
    "$$\n",
    "    SBUR=\\frac{S}{S+I+D}\\times 100\n",
    "$$\n",
    "\n",
    "**Deletion Error Rate(DELR)** and **Insertion Error Rate(INSR)** are defined in a similar manner.\n",
    "\n",
    "\n",
    "**Word accuracy(\\\\(W_{Acc}\\\\))**\n",
    "$$\n",
    "    W_{Acc} = 1 - WER = \\frac{N - S - I - D}{N}\n",
    "$$\n",
    "\n",
    "**Word Correctness(\\\\(W_{Corr}\\\\))**\n",
    "$$\n",
    "    W_{Corr} = \\frac{N-S-D}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Normalized Cross Entropy Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
